---
title: "A Retrospective Analysis of COVID Cases in Connecticut by County"
author:
  - name: "Fadel M. Megahed ^[Email: fmegahed@miamioh.edu | Phone: +1-513-529-4185 | Website: <a href=\"https://miamioh.edu/fsb/directory/?up=/directory/megahefm\">Miami University Official</a>]"
    affiliation: Farmer School of Business, Miami University
  - name: "Allison Jones-Farmer ^[Email: farmerl2@miamioh.edu | Phone: +1-513-529-4823 | Website: <a href=\"https://miamioh.edu/fsb/directory/?up=/directory/farmerl2\">Miami University Official</a>]"
    affiliation: Farmer School of Business, Miami University
  - name: "Steve Rigdon ^[Email: steve.rigdon@slu.edu | Website: <a href=\"https://www.slu.edu/public-health-social-justice/faculty/rigdon-steven.php\">Saint Louis University Official</a>]"
    affiliation: College of  Public Health and Social Justice, Saint Louis University
bibliography: covidRefs.bib
csl: apa.csl
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
    number_sections: TRUE
    theme: simplex
    paged_df: TRUE
    code_folding: show
  includes:
    in_header: structure.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      cache = TRUE,
                      progress = FALSE, 
                      verbose = FALSE,
                      dpi = 600)
options(qwraps2_markup = "markdown")
```

# R Setup and Required Packages
In this project, the open-source R programming language is used to model the progression in the COVID-19 pandemic in different U.S. counties. R is maintained by an international team of developers who make the language available at [The Comprehensive R Archive Network](https://cran.r-project.org/). Readers interested in reusing our code and reproducing our results should have R installed locally on their machines. R can be installed on a number of different operating systems (see [Windows](https://cran.r-project.org/bin/windows/), [Mac](https://cran.r-project.org/bin/macosx/), and [Linux](https://cran.r-project.org/bin/linux/) for the installation instructions for these systems). We also recommend using the RStudio interface for R. The reader can [download RStudio](http://www.rstudio.com/ide) for free by following the instructions at the link. For non-R users, we recommend the [Hands-on Programming with R](https://rstudio-education.github.io/hopr/packages.html) for a brief overview of the software's functionality. Hereafter, we assume that the reader has an introductory understanding of the R programming language.

In the code chunk below, we load the packages used to support our analysis. Note that the code of this and any of the code chunks can be hidden by clicking on the 'Hide' button to facilitate the navigation.

```{r packages}
if(require(checkpoint)==FALSE) install.packages("checkpoint") # check to see if checkpoint is installed; if not, install it
library(checkpoint) # package used to facilitate the reproducibility of our work

# a checkpoint of R packages on CRAN on August 28, 2020 to enable the reproduction of our work in the future
checkpoint("2020-09-15")

# check if packages are not installed; if yes, install missing packages
packages = c("tidyverse", "magrittr", "dataPreparation", "recipes", "doParallel", # data analysis
             "COVID19", # used to obtain county level (can be also used for country level)
             "DT", # for printing nice looking data in R Markdown
             "zoo", "fpp2", #for time series analysis in R,
             "ggdendro", "gridExtra", # for plotting
             "dtwclust", "factoextra", "TSclust", "proxy") # for cluster analysis
newPackages = packages[!(packages %in% installed.packages()[,"Package"])]
if(length(newPackages) > 0) install.packages(newPackages)

# using the library command to load all packages; invisible used to avoid printing all packages and dependencies used
invisible(lapply(packages, library, character.only = TRUE))

set.seed(2020)
sInfo = sessionInfo()
```

# Extracting U.S. Counties' Data & Computing their New Cases/Deaths
In this section, we utilize the [COVID19 package](https://cran.r-project.org/web/packages/COVID19/COVID19.pdf) to obtain the following information: [@Guidotti2020]    

  - Confirmed cases, recoveries and deaths;    
  - policy information (e.g., transport closing, school closing, closing event, movement restrictions, testing policieis, and contact tracing);  
  - Population and standard geographic information for each county; and  
  - Variables captured in both [Apple's](https://www.apple.com/covid19/mobility) and [Google's](https://www.google.com/covid19/mobility/index.html?hl=en) mobility reports.

From this information, we have also computed the new daily and weekly confirmed cases/deaths per county. The data is stored in a tidy format, but can be expanded to a wide format using `pivot_wider()` from the [tidyverse](https://www.tidyverse.org/) package.


## Data based on COVID19 Package

```{r confirmedCases, results='asis'}
appleURL = "https://covid19-static.cdn-apple.com/covid19-mobility-data/2016HotfixDev17/v3/en-us/applemobilitytrends-2020-09-13.csv" # inspected from https://www.apple.com/covid19/mobility
googleURL = "https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv"
counties = covid19(country = "US", 
                   level = 3, # for county
                   start = "2020-02-23", # Last Sunday in Feb (a total of 26 new cases on 2020-03-01 in entire US)
                   end = "2020-09-04", # end Date 
                   amr = appleURL, # apple mobility report data
                   gmr = googleURL, # google's mobility report data
                   wb = NULL, # world bank data not helpful for county level analysis
                   verbose = FALSE)

counties %<>% 
  fastFilterVariables(verbose = FALSE) %>% #dropping invariant columns or bijections
  filter(administrative_area_level_2 == "Connecticut") %>%  # CT only
  filter(!is.na(key_numeric)) %>%  # these are not counties
  group_by(id) %>% # grouping the data by the id column to make computations correct
  arrange(id, date) %>% # to ensure correct calculations
  mutate(day = wday(date, label = TRUE) %>% factor(ordered = F), # day of week
         newCases = c(NA, diff(confirmed)), # computing new daily cases
         newDeaths = c(NA, diff(deaths)) )  # computing new daily deaths 

# manually identifying factor variables
factorVars = c("school_closing", "workplace_closing", "cancel_events",
               "gatherings_restrictions", "transport_closing", "stay_home_restrictions",
               "internal_movement_restrictions", "international_movement_restrictions",
               "information_campaigns", "testing_policy", "contact_tracing")

counties %<>% # converting those variables into character and then factor
  mutate_at(.vars = vars(any_of(factorVars)), .funs = as.character) %>% 
  mutate_at(.vars = vars(any_of(factorVars)), .funs = as.factor)


cat(paste0("At this stage, we have only read the data based on the covid package. The resulting data for CT is stored at an object titled counties, which contains ", nrow(counties), " observations and ",
          ncol(counties), " variables. Note that we have filtered observations that do not have a numeric key and removed some columns that do not add any value to future analysis (e.g., invariant cols)."))
```

## Other Possibly Relevant Data

In the code chunk below, we merge our counties' COVID data with four additional datasets:  

- *Rural/ Underserved Counties:* From the [Consumer Financial Protection Bureau](https://www.consumerfinance.gov/policy-compliance/guidance/mortgage-resources/rural-and-underserved-counties-list/), we have obtained the Final 2020 List titled: *Rural or underserved counties*. Per the website, the procedure for determining the classification of a county is as follows: "Beginning in 2020, the rural or underserved counties lists use a methodology for identifying underserved counties described in the Bureau’s interpretive rule: Truth in Lending Act (Regulation Z); [Determining “Underserved” Areas Using Home Mortgage Disclosure Act Data](https://www.consumerfinance.gov/policy-compliance/rulemaking/final-rules/truth-lending-regulation-z-underserved-areas-home-mortgage-disclosure-act-data/)."   

- Based on @DVN/VOQCHQ_2018, we have obtained the voting results for all counties in the 2016 Presidential elections. The data was used to compute the percentage of total votes that went to President Trump, with the underlying hypothesis that the politicization of COVID response (e.g., perception/willingness to use face masks, policies and the population's reaction to the disease) may be explained by party affiliation.   

- Based on the [following Kaiser Health News Webpage](https://khn.org/news/as-coronavirus-spreads-widely-millions-of-older-americans-live-in-counties-with-no-icu-beds/#lookup), we extracted by county information on: (a) number of ICU beds per 10,000 residents; (b) percent of population aged 60+; and (c) number of ICU beds per 10,000 60+aged residents.  

- Based on the [Census's Small Area Income and Poverty Estimates (SAIPE) Program](https://www.census.gov/programs-surveys/saipe.html), we extracted the estimate for the percent of population in poverty. The estimate is based on 2018 data (released in December 2019). At the time of the start of our analysis, these estimates were the most up to date publicly available data.


```{r otherData, results="asis"}
# [A] Rural or Urban Classification of the County
ru = read.csv("https://www.consumerfinance.gov/documents/8911/cfpb_rural-underserved-list_2020.csv")
ru %<>%  transmute(key_numeric = FIPS.Code,
                countyType = "Rural/Underserved") # creates two vars and drop old vars

counties = merge(counties, ru, by = "key_numeric", all.x = TRUE)
counties$countyType %<>% replace_na("Other")


# [B] 2016 Presidential Elections County Data from Harvard https://doi.org/10.7910/DVN/VOQCHQ
elections = read.csv("../Data/countypres_2000-2016.csv") %>% 
  filter(year == 2016 & party == "republican") %>% 
  mutate(key_numeric = FIPS, 
         percRepVotes = 100*(candidatevotes/totalvotes) ) %>% 
  select(key_numeric, percRepVotes)

counties = merge(counties, elections, by = "key_numeric", all.x = TRUE)

# [C] Hospital Beds by County
# https://khn.org/news/as-coronavirus-spreads-widely-millions-of-older-americans-live-in-counties-with-no-icu-beds/
hospitals = read.csv("../Data/data-FPBfZ.csv") %>% 
  transmute(State = State,
            County = County,
            PercentSeniors = Percent.of.Population.Aged.60.,
            icuBedsPer10000Residents = 10000 * (ICU.Beds/Total.Population),
            icuBedsPer10000Seniors = 10000 * ICU.Beds/Population.Aged.60.
         )

counties = merge(counties, hospitals, 
                 by.x = c("administrative_area_level_2", "administrative_area_level_3"),
                 by.y = c("State", "County"),
                 all.x = TRUE)


# [D] Poverty Estimates
download.file("https://www2.census.gov/programs-surveys/saipe/datasets/2018/2018-state-and-county/est18all.xls", "../Data/est18all.xls", mode = "wb")

poverty = readxl::read_excel("../Data/est18all.xls", skip = 3) %>% 
  transmute(key_numeric = paste0(`State FIPS Code`, `County FIPS Code`) %>% as.numeric,
            povertyPercent = `Poverty Percent, All Ages`)

counties = merge(counties, poverty, by = "key_numeric", all.x = TRUE)

counties %<>% group_by(id) # Needs to be regrouped again after the merge steps 

# Saving the data into RDS and CSV
saveRDS(counties, paste0("../Data/CT-tidy-",Sys.Date(),".rds"))
write.csv(counties, paste0("../Data/CT-tidy-",Sys.Date(),".csv"), 
          row.names = FALSE)

cat(paste0("After merging these four additional datasets with our counties object, we have ", nrow(counties), " observations and ",
          ncol(counties), " variables. Note that the merged data is saved both as an RDS and CSV."))
```

# Data Preparation

## Smoothing Alternatives for the Week-over-Week Confirmed Cases

In the code chunk below, we have generated three smoothed variables for each of the *confirmed* and the *newCases* variables within the `county` data frame. The three smoothed variables are:  

- A 7-day moving average for newCases;  and 
- A 7-day moving median for newCases.

```{r smoothing, results= "asis"}
wowCases = counties %>% 
  select(id, key_google_mobility, population, date, confirmed, newCases) %>%
  arrange(id, date) %>% # to ensure correct calculations
  mutate(
         newMA7 = rollmeanr(newCases, k = 7, fill = NA), # 7-day ma of new (adjusted) cases
         newMM7 = rollmedianr(newCases, k = 7, fill = NA),
         maxMA7 = max(newMA7, na.rm = T),
         scaledNewMA7 = newMA7/maxMA7,
         newMA7ByPop = 10000*newMA7/population,
         newMM7ByPop = 10000*newMM7/population)

saveRDS(wowCases, paste0("../Data/CTwowCases-",Sys.Date(),".rds"))
write.csv(wowCases, paste0("../Data/CTwowCases-",Sys.Date(),".csv"), 
          row.names = FALSE)
```

## Reshaping the Data for Clustering

```{r reshape}
wowPrep = wowCases %>% 
  filter(date >= "2020-03-01") %>% # starting from March 01, 2020 (26 new cases in entire US)
  select(key_google_mobility, date, scaledNewMA7) %>% 
  pivot_wider(names_from = date, values_from = scaledNewMA7)

constantColumns = whichAreConstant(wowPrep, verbose = F) # identifying constant columns
wowPrep %<>% select( -all_of(constantColumns) ) %>%  # speeds up clustering by dec length of series
  as.data.frame()
row.names(wowPrep) = wowPrep[,'key_google_mobility'] # needed for tsclust
wowPrep = wowPrep[,-c(1,2)]
```


# Time-Series Clustering

## Clustering {.tabset .tabset-fade .tabset-pills}


### ACF {-}
```{r acfd}
proxy::pr_DB$set_entry(FUN = diss.ACF, names = c("ACFD"), 
                       loop = TRUE, type = "metric", distance = TRUE, 
                       description = "Autocorrelation-based distance") 

row.names(wowPrep) %<>% str_remove_all("US, Connecticut, ") %>% str_remove_all(" County")

dACF = tsclust(wowPrep, type = "hierarchical", k=2L, distance = "ACFD",
               preproc = NULL, seed = 2020)

hclus = cutree(dACF, k = 2) %>% 
  as.data.frame(.) %>%
  rename(.,cluster_group = .) %>%
  rownames_to_column("County")

hcdata = dendro_data(dACF)
names_order = hcdata$labels$label

p1 = hcdata %>%
  ggdendrogram(., rotate=TRUE, leaf_labels=FALSE)

p2 = wowPrep %>%
  rownames_to_column(var = "County") %>% 
  pivot_longer(cols = starts_with("2020"), names_to = "Date") %>% 
  full_join(., hclus, by = "County") %>% 
  mutate(cluster_group = as.factor(cluster_group)) %>% 
  mutate(County = factor(County, levels = rev(as.character(names_order)))) %>% 
  ggplot(aes(x = Date, y = value, colour = cluster_group, group = County)) +
  geom_line()+
  facet_wrap(~County, ncol = 1, strip.position="left") + 
  guides(color=FALSE) +
  theme_bw() + 
  theme(strip.background = element_blank(), strip.text = element_blank())

gp1 = ggplotGrob(p1)
gp2 = ggplotGrob(p2) 

grid.arrange(gp2, gp1, ncol=2, widths=c(4,2))
```

### Correlation {-}
```{r cor}
proxy::pr_DB$set_entry(FUN = diss.COR, names = c("Cor"), 
                       loop = TRUE, type = "metric", distance = TRUE, 
                       description = "Pearson Correlation-based distance")  

row.names(wowPrep) %<>% str_remove_all("US, Connecticut, ") %>% str_remove_all(" County")

cor = tsclust(wowPrep, type = "hierarchical", k=2L, distance = "Cor",
               preproc = NULL, seed = 2020)

hclus = cutree(cor, k = 2) %>% 
  as.data.frame(.) %>%
  rename(.,cluster_group = .) %>%
  rownames_to_column("County")

hcdata = dendro_data(cor)
names_order = hcdata$labels$label

p1 = hcdata %>%
  ggdendrogram(., rotate=TRUE, leaf_labels=FALSE)

p2 = wowPrep %>%
  rownames_to_column(var = "County") %>% 
  pivot_longer(cols = starts_with("2020"), names_to = "Date") %>% 
  full_join(., hclus, by = "County") %>% 
  mutate(cluster_group = as.factor(cluster_group)) %>% 
  mutate(County = factor(County, levels = rev(as.character(names_order)))) %>% 
  ggplot(aes(x = Date, y = value, colour = cluster_group, group = County)) +
  geom_line()+
  facet_wrap(~County, ncol = 1, strip.position="left") + 
  guides(color=FALSE) +
  theme_bw() + 
  theme(strip.background = element_blank(), strip.text = element_blank())

gp1 = ggplotGrob(p1)
gp2 = ggplotGrob(p2) 

grid.arrange(gp2, gp1, ncol=2, widths=c(4,2))
```


### DTW {-}
```{r dtw}
row.names(wowPrep) %<>% str_remove_all("US, Connecticut, ") %>% str_remove_all(" County") 

dtw = tsclust(wowPrep, type = "hierarchical", k=2L, distance = "dtw", preproc = NULL, seed = 2020) 

hclus = cutree(dtw, k = 2) %>% 
  as.data.frame(.) %>%
  rename(.,cluster_group = .) %>%
  rownames_to_column("County")

hcdata = dendro_data(dtw)
names_order = hcdata$labels$label

p1 = hcdata %>%
  ggdendrogram(., rotate=TRUE, leaf_labels=FALSE)

p2 = wowPrep %>%
  rownames_to_column(var = "County") %>% 
  pivot_longer(cols = starts_with("2020"), names_to = "Date") %>% 
  full_join(., hclus, by = "County") %>% 
  mutate(cluster_group = as.factor(cluster_group)) %>% 
  mutate(County = factor(County, levels = rev(as.character(names_order)))) %>% 
  ggplot(aes(x = Date, y = value, colour = cluster_group, group = County)) +
  geom_line()+
  facet_wrap(~County, ncol = 1, strip.position="left") + 
  guides(color=FALSE) +
  theme_bw() + 
  theme(strip.background = element_blank(), strip.text = element_blank())

gp1 = ggplotGrob(p1)
gp2 = ggplotGrob(p2) 

grid.arrange(gp2, gp1, ncol=2, widths=c(4,2))
```

### Euclidean {-}
```{r euc}
proxy::pr_DB$set_entry(FUN = diss.EUCL, names = c("EUCL"), 
                       loop = TRUE, type = "metric", distance = TRUE, 
                       description = "Euclidean-based distance")  

row.names(wowPrep) %<>% str_remove_all("US, Connecticut, ") %>% str_remove_all(" County")

eucl = tsclust(wowPrep, type = "hierarchical", k=2L, distance = "EUCL",
               preproc = NULL, seed = 2020)

hclus = cutree(eucl, k = 2) %>% 
  as.data.frame(.) %>%
  rename(.,cluster_group = .) %>%
  rownames_to_column("County")

hcdata = dendro_data(eucl)
names_order = hcdata$labels$label

p1 = hcdata %>%
  ggdendrogram(., rotate=TRUE, leaf_labels=FALSE)

p2 = wowPrep %>%
  rownames_to_column(var = "County") %>% 
  pivot_longer(cols = starts_with("2020"), names_to = "Date") %>% 
  full_join(., hclus, by = "County") %>% 
  mutate(cluster_group = as.factor(cluster_group)) %>% 
  mutate(County = factor(County, levels = rev(as.character(names_order)))) %>% 
  ggplot(aes(x = Date, y = value, colour = cluster_group, group = County)) +
  geom_line()+
  facet_wrap(~County, ncol = 1, strip.position="left") + 
  guides(color=FALSE) +
  theme_bw() + 
  theme(strip.background = element_blank(), strip.text = element_blank())

gp1 = ggplotGrob(p1)
gp2 = ggplotGrob(p2) 

grid.arrange(gp2, gp1, ncol=2, widths=c(4,2))
```

### Periodgram Based Dissimilarity {-}
```{r perD}
proxy::pr_DB$set_entry(FUN = diss.PER, names = c("Per"), 
                       loop = TRUE, type = "metric", distance = TRUE, 
                       description = "Periodogram-based distance") 

row.names(wowPrep) %<>% str_remove_all("US, Connecticut, ") %>% str_remove_all(" County")

perD = tsclust(wowPrep, type = "hierarchical", k=2L, distance = "Per",
               preproc = NULL, seed = 2020)

hclus = cutree(perD, k = 2) %>% 
  as.data.frame(.) %>%
  rename(.,cluster_group = .) %>%
  rownames_to_column("County")

hcdata = dendro_data(perD)
names_order = hcdata$labels$label

p1 = hcdata %>%
  ggdendrogram(., rotate=TRUE, leaf_labels=FALSE)

p2 = wowPrep %>%
  rownames_to_column(var = "County") %>% 
  pivot_longer(cols = starts_with("2020"), names_to = "Date") %>% 
  full_join(., hclus, by = "County") %>% 
  mutate(cluster_group = as.factor(cluster_group)) %>% 
  mutate(County = factor(County, levels = rev(as.character(names_order)))) %>% 
  ggplot(aes(x = Date, y = value, colour = cluster_group, group = County)) +
  geom_line()+
  facet_wrap(~County, ncol = 1, strip.position="left") + 
  guides(color=FALSE) +
  theme_bw() + 
  theme(strip.background = element_blank(), strip.text = element_blank())

gp1 = ggplotGrob(p1)
gp2 = ggplotGrob(p2) 

grid.arrange(gp2, gp1, ncol=2, widths=c(4,2)) 
```



<span style="color: blue;"> To be eventually continued!!!</span>


---

# References {-}
