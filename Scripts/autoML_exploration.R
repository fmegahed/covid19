# Clearing the workspace and setting the working directory
rm(list = ls()) # clear workspace
cat("\014") # clear consolde
if(!is.null(dev.list())) dev.off() # clear plots
setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) # set directory

# Loading the required packages
pacman::p_load(tidyverse, magrittr, caret, h2o, bit64, DT, pander)

# read the data (will need to change the extension)
df = readRDS('../Data/df.rds')

df %<>% select(-fips) # remove fips from data frame
df$popDensity = log(df$popDensity) # since it is highly skewed

set.seed(2020) # setting the seed for reproducibility

# Creating training and testing data for ML models
trainRowNums = createDataPartition(df$clustReLeveled, p = 0.8, list = F) %>% 
  as.vector() # index rows such that approximately 80% of each class are selected
trainData = df[trainRowNums,] # training set using the above indices
testData = df[-trainRowNums,] # testing set excluding the above indices

# initializing the h2o cluster
h2o.init()

trainData %<>% as.h2o() 
testData %<>% as.h2o()

y = "clustReLeveled"
testY = testData$clustReLeveled %>% as.vector() %>% as.factor()

predictors = setdiff(names(trainData), y)

# Run AutoML
aml = h2o.automl(x=predictors, y=y, 
                  training_frame = trainData, 
                  seed = 2020,
                  balance_classes = TRUE,
                  stopping_metric = "mean_per_class_error"
)

# Getting the leaderboard/ranking of the models
lb = aml@leaderboard
lb = as.data.frame(lb) # converting the h20 object to data frame

# Cleaning the names of the models
lb$model_id %<>%  str_remove('_AutoML_') %>% # removing _AutoML_ from all names
  str_remove('\\d{8}') %>% #removing date from all names
  str_remove('\\d{6}') %>% # removing autogenerated timestamp (I think 6 digit num) from names
  str_replace('__', '_') %>% #replacing the resulting double __ with a singular _
  str_replace('grid', replacement = 'Grid') %>% # shortening Grid number
  str_replace('__model', replacement = '_Model') # shortening model number
lb$model_id = gsub("_$", "", lb$model_id) # removing trailing '_' in some models

# nicely printing the leaderboard
datatable(lb,
          extensions = c('FixedColumns', 'Buttons'), options = list(
            dom = 'Bfrtip',
            scrollX = TRUE,
            buttons = c('copy', 'csv', 'excel', 'pdf', 'print'),
            fixedColumns = list(leftColumns = 1)),
          rownames = FALSE) %>% 
  formatRound(columns= seq(2,5), digits=3)

# Evaluating the models on the test dataset
testPrediction =  h2o.predict(aml@leader, testData) %>% as.data.frame 
confMatrixAutoML = confusionMatrix(testY, testPrediction$predict) # printing the confusion matrix

# Printing the resulting tables nicely
pander(confMatrixAutoML$table)
pander(confMatrixAutoML$byClass)
pander(confMatrixAutoML$overall)

exa = h2o.explain(aml, testData)
print(exa) # note that this provides an interactive output -- not sure that it would work in a markdown

h2o.shutdown()
